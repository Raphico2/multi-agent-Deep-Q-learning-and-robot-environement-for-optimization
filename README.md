# multi-agent-Deep-Q-learning-and-robot-environement-for-optimization
The code refer to the full integration of a restaurant environment, managed by robots using multi-agent deep Q learning model 

Project
Project Report Cogni've Robo'c
   Our project deals with robot cogniGon and collaboraGon in order to perform opGmizaGon task. The goal was to create a restaurant environment and train robots to manage and opGmize the gesGon of the restaurant. The restaurant is a Pizza fast food where customers enter, order, wait unGl be served and then pay and eat. The robots are divided into two type, waiter robots and cooker robots. They must learn how to cooperate to answer the demand as fast as possible, keep the restaurant clean and then improve the customer saGsfacGon in order to increase the income. The report aims to detail the environment, the robots, the AI model we used and our results.
1. The goal and environment
Environment
The environment is a restaurant that can host 200 customers maximum a day. The restaurant is open for 4 hours which represent in our model 240 Gmestamp. Each day, every customer can decide to come or not, the probability of coming into the restaurant is set following a Bernoulli Law where the probability is the customer self-saGsfacGon. At the beginning, each customer has a probability of 0.5 to come, then, in funcGon of its last experience in the restaurant, its self-saGsfacGon can increase or decrease and then, influence the chances to return the next day. When a customer decides to come, its arrival Gmestamp is set following a normal law with a mean of 120 Gmestamp and a variance of 60 Gmestamp.
The environment is parGally observable for the robots who manage the restaurant because they don’t know customer saGsfacGon, the number of customer who will come and customer arrival Gme. Each day, a scheduler is built and program at what Gme customer will arrive, this informaGon is not accessible for robots.
On the other side, robots must manage the restaurant properly in order to answer the demand as fast as possible. The robots can be either waiter or cooker. The cooker robot has 3 possible acGons, cook a pizza, wash a plate, Recharge itself. Each of those acGons take exactly one Gmestamp. The quanGty of energy used by the robot for cooking or washing is different and the quanGty of energy provided by 1 Gmestamp of charge is a parameter of our model. Waiter robot can take the order of a customer, deliver the pizza, clean the restaurant, and recharge itself. The quanGty of robots of each type can be set as parameter of the model. Each acGon takes one Gmestamp.
Each customer enters the restaurant at a random Gmestamp, then its saGsfacGon is computed by calculaGng the Gme he waited from its arrival unGl being served. If the customer waits more than the average waiGng Gme, the saGsfacGon is decreasing linearly Gmestamp a[er Gmestamp, if he doesn’t get served unGl the end of the day its saGsfacGon is
 
set to 0 and he will never come back. Moreover, the self-saGsfacGon also depends on the cleanness level of the restaurant, each Gme a robot cooks or a waiter delivers, he let a small amount of dirt in the restaurant, the dirt quanGty gives a cleanness score that is mulGplied by the waiGng score to give the final self-saGsfacGon score for a customer.
Each customer has a
Each Gme a customer gets served, he pays 10$, each Gme a cooker robot makes a pizza, it cost 2$. (The price, cost, average waiGng Gme and dirt score level can be set in the configuraGon of the model).
A simulaGon in our environment represents a year simulaGon, where the number of day and hours of acGvity per day can be set in the configuraGon of our environment. For the project, we trained our model on the basis of 4 hours opening Gme (240 Gmestamp) per days and a “year” of 50 days.
Agents
Our agents are the robots, we want to train our agent to manage the restaurant as best as possible. Every robot has a type (cooker or waiter) and pre-defined acGons it can perform as we described above. It is important to noGce that every acGon take one Gmestamp which represents 1 minutes. The acGon of cook gives as result 1 pizza created. On the other side, there is a limited number of plate that need to be washed to be reused, when the cooker robot decides to perform the acGon “Wash”, a defined number of plate are washed in one Gmestamp, this number is a parameter that can be set in the configuraGon. It is the same for the acGon “Clean” of the waiter, the amount of dirt the robot is able to clean in one Gmestamp can also be set in the configuraGon of the environment.
Our model has been trained with 3 cooker robots and 3 waiters robots
Goal and expectaGon
Our goal project is to create a model that will teach our robots to manage the restaurant. The goal of our robots is to maximize the year income of the restaurant. In order to do it, robots have to collaborate to increase as best as possible the customer saGsfacGon, more a customer is saGsfied, more the chance of coming back the next day is high and then, more the daily income will increase. In our project we will use reinforcement learning and implement a mulG-agent Deep Q learning algorithm in order to teach the robots. We expect to find the best policy for our robots to manage the restaurant and reach the maximum possible daily income. Moreover, we want our robots to maximize the customer saGsfacGon and keep the restaurant clean.
2. Our code, model, and algorithm
Code
Our code is divided into 9 python files. 6 of the python files aims to describe the environment, agent, visual render, and funcGons such as acGons, pre-condiGons, and effects. 1 python file is the reinforcement learning algorithm implementaGon, 1 is for training our model and the last one is for showing results. It is important to noGce that we did not used
  
unified planning since it was not appropriate for RL tasks, but we tried to maintain a structure close to unified planning, we use fluent, acGons with precondiGons and effects.
- Config.py : Is a class where we can set all the parameters of the environment. We can set all acGons efficiency, acGon energy cost, maximum number of customers, number hours of opening, average waiGng Gme ... The config file must be instanGated to start the model, it can also allow the user to set the real variables of its restaurant.
- State.py : The file help to maintain the fluent of our model and create a numerical representaGon for each robot state of the word representaGon. In funcGon of the Gme of robot (cooker or waiter) the representaGon of the environment is different. For cooker robots, the state representaGon is the level of bahery of the robot, the number of available plate (cleaned) and the number of ordered pizza that have not been cooked yet. On the other side, the waiter robot has a different state representaGon which contains the number of accumulated dirt in the restaurant, the bahery level, for each customer the status and the waiGng Gme.
- AcGon.py: The file describes the acGon space of the robots. It implements the post condiGons and effect of an acGon, determines what acGon are possible from a given state for each robots and update the fluent once an acGon is performed. An acGon can be done if the current state allows it for a robot, once the acGon has been done, it updates the state and create a new one. Moreover, because there are many robots, the file has a lock funcGon that can allow robots to lock resources at each Gmestamp in order to avoid duplicate uGlizaGon of resources and then to forbid a robot to perform the same acGon of another one. In our environment, 2 waiter robots cannot serve the same customer, or two cooker robot cannot prepare the same order.
- ModelizaGon.py: The file constructs the visual model render of our environment. It is represented like that at each Gmestamp:
     
 At each Gmestamp there are 2 representaGon of the environment. The first one without the acGons chosen and the second one with the acGons chosen. In Yellow the cooker robots, in orange the waiter robots. On the le[ side of the dashboard the bahery level of each robots. On the right side of the dashboard, all the customers present in the restaurant are represented. When a customer is in pink, It means he has not ordered yet, when he is in green it means he is waiGng to be delivered. The number wrihen inside the customer is its waiGng Gme since its arrival. On the right side of the blue rectangle, the number of accumulated dirt, delivered customer, available plate, Pizza ordered and Pizza waiGng to be served are wrihen.
At each Gmestamp, robots are choosing an acGon to perform, this acGon appears on the dashboard at each Gmestamp in a grey rectangle.
- Environement.py : It is the global implementaGon of the environment, it coordinates the state file, the acGon file, the visual model file and the config file. It is implemented as a gym environment with the funcGons step, render and reset. The funcGon step simulates a Gmestamp from the moment each robots get their acGon to perform. The environment class allow the simulator to get all the informaGon of the current global state of the world but also the observaGon space of each robot at each Gmestamp and their acGon space and possible acGons space. We can reset or start a new day from the environment class.
- UGls.py: Contains the funcGon for gekng the possible acGon with the greater Q value for a given robot and the funcGon that determine randomly arrival Gme of each robot.
- DQN.py: The implementaGon of the mulG-agent Deep Q Learning Algorithm for our robots. Each robot has a brain which is Fully connected neural network. The NN receives at each Gmestamp the current observaGon space of the robot and return the Q-Value for each acGons in the robot acGon space. At each Gmestamp, the robot will take the acGon with the highest Q-Value among the acGons that are possible to perform at this Gmestamp. The file also contains a funcGon predict and learn. The funcGon predicts take the current observaGon of each robot and return the Q-values for each robot. The funcGon learn is a funcGon that keeps in memory all the acGons
  
and reward a robot performed in a year and go backward in the NN of each robots to update the robot knowledge.
- Simulator.py: The Simulator is made for training the model during a long period of Gme. During 2000 simulated year containing 50 days each and 240 Gmestamp per days, robots are exploring and then exploiGng their environment by creaGng new strategy and check their efficiency. We implemented and epsilon-greedy policy to make the algorithm explore at the beginning and then exploit more as far we get in the training procedure. The main purpose is to let the robot understand how to maximize the profit in a year. A[er each day, robots received a common reward which is the mean saGsfacGon of all the customers that ate in the restaurant during the day. Because we know that maximizing customer saGsfacGon will increase the number of customer the next day and then increase profits, we thought that this reward is the best way for robots to understand and reach their objecGve. If the customer saGsfacGon is high, the reward will be greater and then the robots will keep the best policy that maximize the customer saGsfacGon.
- TestSimulator.py: The file is the main file to see an example of a day in the model and perform a year simulaGon of the restaurant and see the result in term of day and year income. The file have to be used a[er the training to see if we did obtained good results in our project.
Model and Algorithm
Because mulG-agent reinforcement learning is a very challenging task, especially for robots, we decide to try it for our project. Our algorithm is a Deep Q learning fully decentralized. It means that each robot has its own Neural Network (in our case a fully connected neural network with 2 hidden layers) and each robot takes its own decision regarding its observaGon of the world. Each robot has a parGal observaGon space of the restaurant. The goal of the model is to make the robot learn the best policy to maximize customer saGsfacGon. During the training procedure, the model funcGon “predict” is called at each Gmestamp to give the Q value for each acGon for each robot. The predicGon can be random if the model is exploring the environment or can be done with the Neural Network if the model is exploiGng the environment.
At each Gmestamp, the acGons and the observaGon space for each robot are memorized. At the end of the day, robots are gekng the reward (customer saGsfacGon mean) and then the weight of the Neural network of each robot is updated with backward propagaGon.
A[er the learning procedure, the brains of each robot are saved and then can be reused for tesGng the model.
Reward and punishment
In our RL model we had to define a reward and punishment policy. We wanted at the beginning to set the daily income as reward but we figured out that it was not relevant because customers can pay even if they are not saGsfied. We decided to set the global
    
saGsfacGon as the final reward of the day. We also added small reward during a day to encourage or discourage robots to perform or not perform acGons.
For the cooker robots we applied a penalty if they choose to not do anything if they are things to do, moreover a small penalty is set when the cooker robot decides to recharge if it has enough bahery to perform other acGvity.
For the waiter robot, we give a reward each Gme the robot chooses to deliver or take the order of the customer who is first on the waiGng list. The size of the reward increases if the robot decides to serve or take the order of someone who is on the top of the waiGng list, and get close to 0 if the robot choose to serve someone who is far from the top. We also punish a waiter who is not doing anything if there are things to do and reward them when they clean the restaurant if the dirt level is high.
Results
We trained our algorithm during 10000 simulated years or 325 days with 240 Gmestamp per day. The tradeoff between exploraGon and exploitaGon is decreasing over Gmestamp and reach a level of 10%-90% a[er the year 7000. The training Gme of our algorithm was very long, it took us 6 days to complete the training part. Each simulated year take approximaGvely 1 minutes. A[er the training part, we get the following results:
- The Robots succeeded to deal with an amount of 150 customers a day and get them all saGsfied with a saGsfacGon score of 0.8.
- The daily income reach a[er the year 8000 an amount between 1100$ and 1200$ (when 1200 is the maximum possible amount since there can be maximum 150 customers in the restaurant)
- The Restaurant is almost all the Gme above the first level of dirt.
- The robots succeeded to learn that they need all the Gme bahery and go recharge
themselves when there amount of bahery gets lower.
- A[er the year 4500 robots understood that in order to maximize the customer
saGsfacGon, all the customers must be served a[er the end of the day.
- A[er the year 6350 robots understood that the best policy is to serve the customer
which come first in the restaurant first.
The training was really saGsfying for the amount of 200 customers maximum for 6 robots in the restaurant. We also saw that Robots improved their coordinaGon, for example, 2 of the cooker robots are almost all the Gme cooking where 1 is almost all the Gme washing the plate.
A[er training, we make a record of a day and a year of simulaGon with the visualizaGon in order to show our results. NoGce that we can show our results and modifying parameters directly on the code in the file simulator.py.
We also collected the results over a year and see how many customers the restaurant arrives to maintains. With our environment algorithm, more a customer is saGsfied from its last visit, more the chances to come back the next day are high. For an amount of 200 customers maximum and a saGsfacGon rate of 0.5 at the beginning, the robots start with a number of customer approximaGvely equal to 100. They get to answer the demand for all customer to increase their saGsfacGon to get more customers the next day, then they have to again
 
answer the demand. We noGced that when the number of customer in the restaurant is lower than 125, the customer saGsfacGon lay between 0.53 and 0.65 which is beher than the beginning, we also noGced that the robots arrive to answer the demand for all customer, nobody is not served a[er the end of the service.
When the number of customers is higher than 125, robots have difficulGes to maintain the saGsfacGon level above 0.5 and to serve all the customers.
A[er a year of service, the customer saGsfacGon lay between 0.55 and 0.6 and the average number of customer a day is 123. Moreover, the daily income of the restaurant is 915$ of profits which is a very good results.
Here is a link for showing a short video for showing the results :
hhps://youtu.be/e6wnH9OFvDk
Anyone can also run the code in the file simulator.py and see the results directly
Limits and improvement
The results showed us that using reinforcement Learning for the gesGon of a restaurant by robots can help improve results in the domain. Our results showed that this type of algorithm works and give to robots an intelligence that could help resolving complex tasks. However, we met some issues such as the fact that robots did not get to predict the probability law of customers arrival (normal law with mean 120 and variance 60). They don’t really act in funcGon of their arrival but in funcGon of which customer is here. Another issue is that robots seem to take order and deliver randomly to customer and not by their arrival Gme.
We think that more training and more exploraGon would help to improve results. We also have a limitaGon for the number of customers the robots get to handle. We see our limitaGon at 125 customers a day. We think that this limitaGon could be passed with more training or increasing the number of robots in the restaurant.
